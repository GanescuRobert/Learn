{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "a51c484ec4fef5f002c8f467736c617cd38a373e870ae09731876c8e0d970dd0"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type this in cmd  < wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\" >\n",
    "import gensim, logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-01-24 20:35:11,002 : INFO : loading projection weights from GoogleNews-vectors-negative300.bin.gz\n",
      "2021-01-24 20:35:50,623 : INFO : loaded (3000000, 300) matrix from GoogleNews-vectors-negative300.bin.gz\n"
     ]
    }
   ],
   "source": [
    "a= 'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz'\n",
    "b= 'GoogleNews-vectors-negative300.bin.gz'\n",
    "gmodel = gensim.models.KeyedVectors.load_word2vec_format(b,binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 0.0123291 ,  0.20410156, -0.28515625,  0.21679688,  0.11816406,\n",
       "        0.08300781,  0.04980469, -0.00952148,  0.22070312, -0.12597656,\n",
       "        0.08056641, -0.5859375 , -0.00445557, -0.296875  , -0.01312256,\n",
       "       -0.08349609,  0.05053711,  0.15136719, -0.44921875, -0.0135498 ,\n",
       "        0.21484375, -0.14746094,  0.22460938, -0.125     , -0.09716797,\n",
       "        0.24902344, -0.2890625 ,  0.36523438,  0.41210938, -0.0859375 ,\n",
       "       -0.07861328, -0.19726562, -0.09082031, -0.14160156, -0.10253906,\n",
       "        0.13085938, -0.00346375,  0.07226562,  0.04418945,  0.34570312,\n",
       "        0.07470703, -0.11230469,  0.06738281,  0.11230469,  0.01977539,\n",
       "       -0.12353516,  0.20996094, -0.07226562, -0.02783203,  0.05541992,\n",
       "       -0.33398438,  0.08544922,  0.34375   ,  0.13964844,  0.04931641,\n",
       "       -0.13476562,  0.16308594, -0.37304688,  0.39648438,  0.10693359,\n",
       "        0.22167969,  0.21289062, -0.08984375,  0.20703125,  0.08935547,\n",
       "       -0.08251953,  0.05957031,  0.10205078, -0.19238281, -0.09082031,\n",
       "        0.4921875 ,  0.03955078, -0.07080078, -0.0019989 , -0.23046875,\n",
       "        0.25585938,  0.08984375, -0.10644531,  0.00105286, -0.05883789,\n",
       "        0.05102539, -0.0291748 ,  0.19335938, -0.14160156, -0.33398438,\n",
       "        0.08154297, -0.27539062,  0.10058594, -0.10449219, -0.12353516,\n",
       "       -0.140625  ,  0.03491211, -0.11767578, -0.1796875 , -0.21484375,\n",
       "       -0.23828125,  0.08447266, -0.07519531, -0.25976562, -0.21289062,\n",
       "       -0.22363281, -0.09716797,  0.11572266,  0.15429688,  0.07373047,\n",
       "       -0.27539062,  0.14257812, -0.0201416 ,  0.10009766, -0.19042969,\n",
       "       -0.09375   ,  0.14160156,  0.17089844,  0.3125    , -0.16699219,\n",
       "       -0.08691406, -0.05004883, -0.24902344, -0.20800781, -0.09423828,\n",
       "       -0.12255859, -0.09472656, -0.390625  , -0.06640625, -0.31640625,\n",
       "        0.10986328, -0.00156403,  0.04345703,  0.15625   , -0.18945312,\n",
       "       -0.03491211,  0.03393555, -0.14453125,  0.01611328, -0.14160156,\n",
       "       -0.02392578,  0.01501465,  0.07568359,  0.10742188,  0.12695312,\n",
       "        0.10693359, -0.01184082, -0.24023438,  0.0291748 ,  0.16210938,\n",
       "        0.19921875, -0.28125   ,  0.16699219, -0.11621094, -0.25585938,\n",
       "        0.38671875, -0.06640625, -0.4609375 , -0.06176758, -0.14453125,\n",
       "       -0.11621094,  0.05688477,  0.03588867, -0.10693359,  0.18847656,\n",
       "       -0.16699219, -0.01794434,  0.10986328, -0.12353516, -0.16308594,\n",
       "       -0.14453125,  0.12890625,  0.11523438,  0.13671875,  0.05688477,\n",
       "       -0.08105469, -0.06152344, -0.06689453,  0.27929688, -0.19628906,\n",
       "        0.07226562,  0.12304688, -0.20996094, -0.22070312,  0.21386719,\n",
       "       -0.1484375 , -0.05932617,  0.05224609,  0.06445312, -0.02636719,\n",
       "        0.13183594,  0.19433594,  0.27148438,  0.18652344,  0.140625  ,\n",
       "        0.06542969, -0.14453125,  0.05029297,  0.08837891,  0.12255859,\n",
       "        0.26757812,  0.0534668 , -0.32226562, -0.20703125,  0.18164062,\n",
       "        0.04418945, -0.22167969, -0.13769531, -0.04174805, -0.00286865,\n",
       "        0.04077148,  0.07275391, -0.08300781,  0.08398438, -0.3359375 ,\n",
       "       -0.40039062,  0.01757812, -0.18652344, -0.0480957 , -0.19140625,\n",
       "        0.10107422,  0.09277344, -0.30664062, -0.19921875, -0.0168457 ,\n",
       "        0.12207031,  0.14648438, -0.12890625, -0.23535156, -0.05371094,\n",
       "       -0.06640625,  0.06884766, -0.03637695,  0.2109375 , -0.06005859,\n",
       "        0.19335938,  0.05151367, -0.05322266,  0.02893066, -0.27539062,\n",
       "        0.08447266,  0.328125  ,  0.01818848,  0.01495361,  0.04711914,\n",
       "        0.37695312, -0.21875   , -0.03393555,  0.01116943,  0.36914062,\n",
       "        0.02160645,  0.03466797,  0.07275391,  0.16015625, -0.16503906,\n",
       "       -0.296875  ,  0.15039062, -0.29101562,  0.13964844,  0.00448608,\n",
       "        0.171875  , -0.21972656,  0.09326172, -0.19042969,  0.01599121,\n",
       "       -0.09228516,  0.15722656, -0.14160156, -0.0534668 ,  0.03613281,\n",
       "        0.23632812, -0.15136719, -0.00689697, -0.27148438, -0.07128906,\n",
       "       -0.16503906,  0.18457031, -0.08398438,  0.18554688,  0.11669922,\n",
       "        0.02758789, -0.04760742,  0.17871094,  0.06542969, -0.03540039,\n",
       "        0.22949219,  0.02697754, -0.09765625,  0.26953125,  0.08349609,\n",
       "       -0.13085938, -0.10107422, -0.00738525,  0.07128906,  0.14941406,\n",
       "       -0.20605469,  0.18066406, -0.15820312,  0.05932617,  0.28710938,\n",
       "       -0.04663086,  0.15136719,  0.4921875 , -0.27539062,  0.05615234],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "gmodel['cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-0.19140625, -0.04296875,  0.27539062,  0.00488281, -0.3203125 ,\n",
       "        0.08203125,  0.05566406, -0.03613281, -0.31445312,  0.10693359,\n",
       "       -0.359375  ,  0.29882812,  0.02331543,  0.05517578, -0.140625  ,\n",
       "        0.1953125 , -0.23632812, -0.22167969, -0.06542969, -0.3359375 ,\n",
       "        0.25195312, -0.09326172,  0.54296875,  0.11328125, -0.28710938,\n",
       "       -0.12011719, -0.11181641,  0.20996094, -0.33203125,  0.30273438,\n",
       "       -0.3359375 , -0.12255859,  0.12890625, -0.28515625, -0.04223633,\n",
       "        0.25585938,  0.3203125 ,  0.07177734,  0.19042969, -0.01379395,\n",
       "        0.16992188, -0.22460938,  0.5078125 ,  0.08398438, -0.07519531,\n",
       "       -0.06396484,  0.05371094,  0.34570312,  0.46289062, -0.16699219,\n",
       "       -0.30664062,  0.15234375, -0.09765625, -0.26171875, -0.14160156,\n",
       "        0.2265625 ,  0.49609375, -0.10791016, -0.08447266,  0.234375  ,\n",
       "        0.04931641, -0.07128906,  0.05273438, -0.11914062,  0.09814453,\n",
       "        0.11181641, -0.13574219, -0.46875   ,  0.26171875,  0.12158203,\n",
       "        0.31445312,  0.05810547,  0.0703125 , -0.10107422, -0.27734375,\n",
       "       -0.16796875, -0.07128906, -0.08007812,  0.07226562, -0.1484375 ,\n",
       "        0.22949219,  0.03686523, -0.03857422,  0.00616455, -0.12255859,\n",
       "       -0.01940918, -0.0625    ,  0.26953125,  0.34179688, -0.00427246,\n",
       "       -0.49023438, -0.38867188, -0.24316406,  0.12304688,  0.07421875,\n",
       "       -0.25195312,  0.14941406, -0.265625  ,  0.30859375, -0.05834961,\n",
       "       -0.19726562, -0.14941406,  0.01031494, -0.07275391, -0.23632812,\n",
       "       -0.1484375 ,  0.3046875 , -0.10351562,  0.69140625, -0.29492188,\n",
       "       -0.25976562, -0.29296875,  0.31445312, -0.11083984, -0.5       ,\n",
       "       -0.04443359,  0.10058594,  0.04858398, -0.0625    ,  0.02001953,\n",
       "        0.08837891, -0.10058594, -0.00113678,  0.390625  ,  0.16894531,\n",
       "        0.01318359,  0.15625   ,  0.10595703,  0.29296875,  0.18457031,\n",
       "        0.13867188,  0.15820312, -0.38671875,  0.20214844,  0.17285156,\n",
       "        0.578125  , -0.05029297, -0.34179688,  0.46875   ,  0.34765625,\n",
       "        0.23535156, -0.20996094, -0.32226562, -0.26367188, -0.02160645,\n",
       "        0.50390625, -0.31054688, -0.2265625 ,  0.31640625, -0.14550781,\n",
       "        0.05639648, -0.18847656,  0.05126953,  0.73828125,  0.04174805,\n",
       "       -0.02392578, -0.375     , -0.18847656, -0.0625    ,  0.50390625,\n",
       "       -0.02844238, -0.05615234,  0.26367188, -0.0612793 ,  0.06030273,\n",
       "       -0.171875  , -0.00866699,  0.20019531, -0.03173828, -0.18164062,\n",
       "        0.40429688, -0.03710938, -0.03515625,  0.18261719, -0.47070312,\n",
       "       -0.07275391,  0.32617188, -0.17285156, -0.22265625,  0.27929688,\n",
       "       -0.57421875,  0.07275391,  0.20898438, -0.30273438,  0.19335938,\n",
       "        0.04907227, -0.15820312, -0.17675781,  0.18066406,  0.42773438,\n",
       "       -0.25390625,  0.29101562, -0.04760742, -0.28710938, -0.08837891,\n",
       "        0.28710938, -0.3828125 , -0.13574219,  0.05297852, -0.22265625,\n",
       "       -0.09179688, -0.06738281, -0.53125   ,  0.06933594,  0.02514648,\n",
       "        0.04443359, -0.18457031, -0.31054688,  0.02856445,  0.16992188,\n",
       "        0.01196289, -0.12109375,  0.00430298,  0.171875  ,  0.06640625,\n",
       "       -0.02954102, -0.39453125,  0.515625  ,  0.2109375 ,  0.03637695,\n",
       "       -0.390625  , -0.04980469, -0.13378906, -0.19140625, -0.34375   ,\n",
       "       -0.21289062,  0.375     ,  0.03039551,  0.1796875 , -0.37109375,\n",
       "        0.07763672, -0.07666016,  0.07910156, -0.15234375, -0.15429688,\n",
       "       -0.15039062,  0.12304688,  0.1796875 , -0.19335938,  0.125     ,\n",
       "        0.17285156,  0.03173828,  0.24121094, -0.34765625, -0.15136719,\n",
       "       -0.09619141,  0.09326172,  0.13964844, -0.20410156, -0.3671875 ,\n",
       "        0.02368164, -0.24804688,  0.17578125,  0.22460938,  0.18652344,\n",
       "       -0.44921875, -0.1640625 , -0.08837891,  0.1484375 , -0.0480957 ,\n",
       "        0.23144531, -0.06176758, -0.53125   , -0.23730469, -0.26953125,\n",
       "        0.02770996, -0.09667969, -0.05249023, -0.00543213, -0.05981445,\n",
       "        0.3203125 ,  0.34570312, -0.06591797, -0.05786133,  0.54296875,\n",
       "       -0.17675781, -0.05810547, -0.0378418 , -0.14550781, -0.265625  ,\n",
       "       -0.01153564,  0.3515625 ,  0.2421875 ,  0.24023438,  0.01025391,\n",
       "       -0.31445312, -0.20410156, -0.02172852, -0.21386719,  0.52734375,\n",
       "        0.12451172,  0.16113281, -0.46289062,  0.05126953, -0.02648926,\n",
       "       -0.546875  ,  0.30859375, -0.0534668 , -0.34765625,  0.3984375 ],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "gmodel['spatula']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.76094574"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "gmodel.similarity('cat','dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.5868356"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "gmodel.similarity('rabbit','dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.3550165"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "gmodel.similarity('frog','dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.11644859"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "gmodel.similarity('frog','chair')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.12412613"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "gmodel.similarity('cat','spatula')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(sent):\n",
    "    sent = sent.lower()\n",
    "    sent = re.sub(r'<[^>]+>', ' ', sent) # strip html tags\n",
    "    sent = re.sub(r'(\\w)\\'(\\w)', '\\1\\2', sent) # remove apostrophes\n",
    "    sent = re.sub(r'\\W', ' ', sent) # remove punctuation\n",
    "    sent = re.sub(r'\\s+', ' ', sent) # remove repeated spaces\n",
    "    sent = sent.strip()\n",
    "    return sent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "unsup_sentences = []\n",
    "\n",
    "# source: http://ai.stanford.edu/~amaas/data/sentiment/ data from IMDB\n",
    "\n",
    "for dirname in ['train/pos','train/neg','train/unsup','test/pos','test/neg']:\n",
    "    for fname in sorted(os.listdir('aclImdb/'+dirname)):\n",
    "        if fname[-4:] == '.txt':\n",
    "            with open('aclImdb/' + dirname +'/'+fname,encoding='UTF-8') as f:\n",
    "                sent = f.read()\n",
    "                words = extract_words(sent)\n",
    "                unsup_sentences.append(TaggedDocument(words, [dirname+'/'+fname]))\n",
    "\n",
    "# source: http://www.cs.cornell.edu/people/pabo/movie-review-data/\n",
    "\n",
    "for dirname in ['txt_sentoken/pos','txt_sentoken/neg']:\n",
    "    for fname in sorted(os.listdir(dirname)):\n",
    "        if fname[-4:] == '.txt':\n",
    "            with open(dirname + '/' + fname, encoding='UTF-8') as f:\n",
    "                for i,sent in enumerate(f):\n",
    "                    words = extract_words(sent)\n",
    "                    unsup_sentences.append(TaggedDocument(words,['%s/%s-%d' %(dirname,fname,i)]))\n",
    "\n",
    "# source: https://nlp.stanford.edu/sentiment/, data from Rotten Tomatoes\n",
    "\n",
    "with open('stanfordSentimentTreebank/original_rt_snippets.txt',encoding='UTF-8') as f:\n",
    "    for i,line in enumerate(f):\n",
    "        words = extract_words(sent)\n",
    "        unsup_sentences.append(TaggedDocument(words,['rt-%d'% i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "175321"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "len(unsup_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', 'teachers', 'the', 'scramble', 'to', 'survive', 'financially', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', 'teachers', 'pomp', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', 'all', 'remind', 'me', 'of', 'the', 'schools', 'i', 'knew', 'and', 'their', 'students', 'when', 'i', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', 'i', 'immediately', 'recalled', 'at', 'high', 'a', 'classic', 'line', 'inspector', 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', 'student', 'welcome', 'to', 'bromwell', 'high', 'i', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'bromwell', 'high', 'is', 'far', 'fetched', 'what', 'a', 'pity', 'that', 'it', 'is'], tags=['train/pos/0_9.txt']),\n",
       " TaggedDocument(words=['homelessness', 'or', 'houselessness', 'as', 'george', 'carlin', 'stated', 'has', 'been', 'an', 'issue', 'for', 'years', 'but', 'never', 'a', 'plan', 'to', 'help', 'those', 'on', 'the', 'street', 'that', 'were', 'once', 'considered', 'human', 'who', 'did', 'everything', 'from', 'going', 'to', 'school', 'work', 'or', 'vote', 'for', 'the', 'matter', 'most', 'people', 'think', 'of', 'the', 'homeless', 'as', 'just', 'a', 'lost', 'cause', 'while', 'worrying', 'about', 'things', 'such', 'as', 'racism', 'the', 'war', 'on', 'iraq', 'pressuring', 'kids', 'to', 'succeed', 'technology', 'the', 'elections', 'inflation', 'or', 'worrying', 'if', 'the', 'l', 'be', 'next', 'to', 'end', 'up', 'on', 'the', 'streets', 'but', 'what', 'if', 'you', 'were', 'given', 'a', 'bet', 'to', 'live', 'on', 'the', 'streets', 'for', 'a', 'month', 'without', 'the', 'luxuries', 'you', 'once', 'had', 'from', 'a', 'home', 'the', 'entertainment', 'sets', 'a', 'bathroom', 'pictures', 'on', 'the', 'wall', 'a', 'computer', 'and', 'everything', 'you', 'once', 'treasure', 'to', 'see', 'what', 'i', 'like', 'to', 'be', 'homeless', 'that', 'is', 'goddard', 'bol', 'lesson', 'mel', 'brooks', 'who', 'directs', 'who', 'stars', 'as', 'bolt', 'plays', 'a', 'rich', 'man', 'who', 'has', 'everything', 'in', 'the', 'world', 'until', 'deciding', 'to', 'make', 'a', 'bet', 'with', 'a', 'sissy', 'rival', 'jeffery', 'tambor', 'to', 'see', 'if', 'he', 'can', 'live', 'in', 'the', 'streets', 'for', 'thirty', 'days', 'without', 'the', 'luxuries', 'if', 'bolt', 'succeeds', 'he', 'can', 'do', 'what', 'he', 'wants', 'with', 'a', 'future', 'project', 'of', 'making', 'more', 'buildings', 'the', 'be', 'on', 'where', 'bolt', 'is', 'thrown', 'on', 'the', 'street', 'with', 'a', 'bracelet', 'on', 'his', 'leg', 'to', 'monitor', 'his', 'every', 'move', 'where', 'he', 'ca', 'step', 'off', 'the', 'sidewalk', 'h', 'given', 'the', 'nickname', 'pepto', 'by', 'a', 'vagrant', 'after', 'i', 'written', 'on', 'his', 'forehead', 'where', 'bolt', 'meets', 'other', 'characters', 'including', 'a', 'woman', 'by', 'the', 'name', 'of', 'molly', 'lesley', 'ann', 'warren', 'an', 'ex', 'dancer', 'who', 'got', 'divorce', 'before', 'losing', 'her', 'home', 'and', 'her', 'pals', 'sailor', 'howard', 'morris', 'and', 'fumes', 'teddy', 'wilson', 'who', 'are', 'already', 'used', 'to', 'the', 'streets', 'the', 'e', 'survivors', 'bolt', 'is', 'h', 'not', 'used', 'to', 'reaching', 'mutual', 'agreements', 'like', 'he', 'once', 'did', 'when', 'being', 'rich', 'where', 'i', 'fight', 'or', 'flight', 'kill', 'or', 'be', 'killed', 'while', 'the', 'love', 'connection', 'between', 'molly', 'and', 'bolt', 'was', 'necessary', 'to', 'plot', 'i', 'found', 'life', 'stinks', 'to', 'be', 'one', 'of', 'mel', 'brooks', 'observant', 'films', 'where', 'prior', 'to', 'being', 'a', 'comedy', 'it', 'shows', 'a', 'tender', 'side', 'compared', 'to', 'his', 'slapstick', 'work', 'such', 'as', 'blazing', 'saddles', 'young', 'frankenstein', 'or', 'spaceballs', 'for', 'the', 'matter', 'to', 'show', 'what', 'i', 'like', 'having', 'something', 'valuable', 'before', 'losing', 'it', 'the', 'next', 'day', 'or', 'on', 'the', 'other', 'hand', 'making', 'a', 'stupid', 'bet', 'like', 'all', 'rich', 'people', 'do', 'when', 'they', 'do', 'know', 'what', 'to', 'do', 'with', 'their', 'money', 'maybe', 'they', 'should', 'give', 'it', 'to', 'the', 'homeless', 'instead', 'of', 'using', 'it', 'like', 'monopoly', 'money', 'or', 'maybe', 'this', 'film', 'will', 'inspire', 'you', 'to', 'help', 'others'], tags=['train/pos/10000_8.txt']),\n",
       " TaggedDocument(words=['brilliant', 'over', 'acting', 'by', 'lesley', 'ann', 'warren', 'best', 'dramatic', 'hobo', 'lady', 'i', 'have', 'ever', 'seen', 'and', 'love', 'scenes', 'in', 'clothes', 'warehouse', 'are', 'second', 'to', 'none', 'the', 'corn', 'on', 'face', 'is', 'a', 'classic', 'as', 'good', 'as', 'anything', 'in', 'blazing', 'saddles', 'the', 'take', 'on', 'lawyers', 'is', 'also', 'superb', 'after', 'being', 'accused', 'of', 'being', 'a', 'turncoat', 'selling', 'out', 'his', 'boss', 'and', 'being', 'dishonest', 'the', 'lawyer', 'of', 'pepto', 'bolt', 'shrugs', 'indifferently', 'a', 'lawyer', 'he', 'says', 'three', 'funny', 'words', 'jeffrey', 'tambor', 'a', 'favorite', 'from', 'the', 'later', 'larry', 'sanders', 'show', 'is', 'fantastic', 'here', 'too', 'as', 'a', 'mad', 'millionaire', 'who', 'wants', 'to', 'crush', 'the', 'ghetto', 'his', 'character', 'is', 'more', 'malevolent', 'than', 'usual', 'the', 'hospital', 'scene', 'and', 'the', 'scene', 'where', 'the', 'homeless', 'invade', 'a', 'demolition', 'site', 'are', 'all', 'time', 'classics', 'look', 'for', 'the', 'legs', 'scene', 'and', 'the', 'two', 'big', 'diggers', 'fighting', 'one', 'bleeds', 'this', 'movie', 'gets', 'better', 'each', 'time', 'i', 'see', 'it', 'which', 'is', 'quite', 'often'], tags=['train/pos/10001_10.txt']),\n",
       " TaggedDocument(words=['this', 'is', 'easily', 'the', 'most', 'underrated', 'film', 'inn', 'the', 'brooks', 'cannon', 'sure', 'its', 'flawed', 'it', 'does', 'not', 'give', 'a', 'realistic', 'view', 'of', 'homelessness', 'unlike', 'say', 'how', 'citizen', 'kane', 'gave', 'a', 'realistic', 'view', 'of', 'lounge', 'singers', 'or', 'titanic', 'gave', 'a', 'realistic', 'view', 'of', 'italians', 'you', 'idiots', 'many', 'of', 'the', 'jokes', 'fall', 'flat', 'but', 'still', 'this', 'film', 'is', 'very', 'lovable', 'in', 'a', 'way', 'many', 'comedies', 'are', 'not', 'and', 'to', 'pull', 'that', 'off', 'in', 'a', 'story', 'about', 'some', 'of', 'the', 'most', 'traditionally', 'reviled', 'members', 'of', 'society', 'is', 'truly', 'impressive', 'its', 'not', 'the', 'fisher', 'king', 'but', 'its', 'not', 'crap', 'either', 'my', 'only', 'complaint', 'is', 'that', 'brooks', 'should', 'have', 'cast', 'someone', 'else', 'in', 'the', 'lead', 'i', 'love', 'mel', 'as', 'a', 'director', 'and', 'writer', 'not', 'so', 'much', 'as', 'a', 'lead'], tags=['train/pos/10002_7.txt']),\n",
       " TaggedDocument(words=['this', 'is', 'not', 'the', 'typical', 'mel', 'brooks', 'film', 'it', 'was', 'much', 'less', 'slapstick', 'than', 'most', 'of', 'his', 'movies', 'and', 'actually', 'had', 'a', 'plot', 'that', 'was', 'followable', 'leslie', 'ann', 'warren', 'made', 'the', 'movie', 'she', 'is', 'such', 'a', 'fantastic', 'under', 'rated', 'actress', 'there', 'were', 'some', 'moments', 'that', 'could', 'have', 'been', 'fleshed', 'out', 'a', 'bit', 'more', 'and', 'some', 'scenes', 'that', 'could', 'probably', 'have', 'been', 'cut', 'to', 'make', 'the', 'room', 'to', 'do', 'so', 'but', 'all', 'in', 'all', 'this', 'is', 'worth', 'the', 'price', 'to', 'rent', 'and', 'see', 'it', 'the', 'acting', 'was', 'good', 'overall', 'brooks', 'himself', 'did', 'a', 'good', 'job', 'without', 'his', 'characteristic', 'speaking', 'to', 'directly', 'to', 'the', 'audience', 'again', 'warren', 'was', 'the', 'best', 'actor', 'in', 'the', 'movie', 'but', 'fume', 'and', 'sailor', 'both', 'played', 'their', 'parts', 'well'], tags=['train/pos/10003_8.txt']),\n",
       " TaggedDocument(words=['this', 'is', 'the', 'comedic', 'robin', 'williams', 'nor', 'is', 'it', 'the', 'quirky', 'insane', 'robin', 'williams', 'of', 'recent', 'thriller', 'fame', 'this', 'is', 'a', 'hybrid', 'of', 'the', 'classic', 'drama', 'without', 'over', 'dramatization', 'mixed', 'with', 'robi', 'new', 'love', 'of', 'the', 'thriller', 'but', 'this', 'is', 'a', 'thriller', 'per', 'se', 'this', 'is', 'more', 'a', 'mystery', 'suspense', 'vehicle', 'through', 'which', 'williams', 'attempts', 'to', 'locate', 'a', 'sick', 'boy', 'and', 'his', 'keeper', 'also', 'starring', 'sandra', 'oh', 'and', 'rory', 'culkin', 'this', 'suspense', 'drama', 'plays', 'pretty', 'much', 'like', 'a', 'news', 'report', 'until', 'willia', 'character', 'gets', 'close', 'to', 'achieving', 'his', 'goal', 'i', 'must', 'say', 'that', 'i', 'was', 'highly', 'entertained', 'though', 'this', 'movie', 'fails', 'to', 'teach', 'guide', 'inspect', 'or', 'amuse', 'it', 'felt', 'more', 'like', 'i', 'was', 'watching', 'a', 'guy', 'williams', 'as', 'he', 'was', 'actually', 'performing', 'the', 'actions', 'from', 'a', 'third', 'person', 'perspective', 'in', 'other', 'words', 'it', 'felt', 'real', 'and', 'i', 'was', 'able', 'to', 'subscribe', 'to', 'the', 'premise', 'of', 'the', 'story', 'all', 'in', 'all', 'i', 'worth', 'a', 'watch', 'though', 'i', 'definitely', 'not', 'friday', 'saturday', 'night', 'fare', 'it', 'rates', 'a', '7', '7', '10', 'from', 'the', 'fiend'], tags=['train/pos/10004_8.txt']),\n",
       " TaggedDocument(words=['yes', 'its', 'an', 'art', 'to', 'successfully', 'make', 'a', 'slow', 'paced', 'thriller', 'the', 'story', 'unfolds', 'in', 'nice', 'volumes', 'while', 'you', 'do', 'even', 'notice', 'it', 'happening', 'fine', 'performance', 'by', 'robin', 'williams', 'the', 'sexuality', 'angles', 'in', 'the', 'film', 'can', 'seem', 'unnecessary', 'and', 'can', 'probably', 'affect', 'how', 'much', 'you', 'enjoy', 'the', 'film', 'however', 'the', 'core', 'plot', 'is', 'very', 'engaging', 'the', 'movie', 'does', 'rush', 'onto', 'you', 'and', 'still', 'grips', 'you', 'enough', 'to', 'keep', 'you', 'wondering', 'the', 'direction', 'is', 'good', 'use', 'of', 'lights', 'to', 'achieve', 'desired', 'affects', 'of', 'suspense', 'and', 'unexpectedness', 'is', 'good', 'very', 'nice', '1', 'time', 'watch', 'if', 'you', 'are', 'looking', 'to', 'lay', 'back', 'and', 'hear', 'a', 'thrilling', 'short', 'story'], tags=['train/pos/10005_7.txt']),\n",
       " TaggedDocument(words=['in', 'this', 'critically', 'acclaimed', 'psychological', 'thriller', 'based', 'on', 'true', 'events', 'gabriel', 'robin', 'williams', 'a', 'celebrated', 'writer', 'and', 'late', 'night', 'talk', 'show', 'host', 'becomes', 'captivated', 'by', 'the', 'harrowing', 'story', 'of', 'a', 'young', 'listener', 'and', 'his', 'adoptive', 'mother', 'toni', 'collette', 'when', 'troubling', 'questions', 'arise', 'about', 'this', 'bo', 'story', 'however', 'gabriel', 'finds', 'himself', 'drawn', 'into', 'a', 'widening', 'mystery', 'that', 'hides', 'a', 'deadly', 'secret', 'according', 'to', 'fil', 'official', 'synopsis', 'you', 'really', 'should', 'stop', 'reading', 'these', 'comments', 'and', 'watch', 'the', 'film', 'now', 'the', 'how', 'did', 'he', 'lose', 'his', 'leg', 'ending', 'with', 'ms', 'collette', 'planning', 'her', 'new', 'life', 'should', 'be', 'chopped', 'off', 'and', 'sent', 'to', 'deleted', 'scenes', 'land', 'i', 'overkill', 'the', 'true', 'nature', 'of', 'her', 'physical', 'and', 'mental', 'ailments', 'should', 'be', 'obvious', 'by', 'the', 'time', 'mr', 'williams', 'returns', 'to', 'new', 'york', 'possibly', 'her', 'blindness', 'could', 'be', 'in', 'question', 'but', 'a', 'revelation', 'could', 'have', 'be', 'made', 'certain', 'in', 'either', 'the', 'highway', 'or', 'video', 'tape', 'scenes', 'the', 'film', 'would', 'benefit', 'from', 'a', 're', 'editing', 'how', 'about', 'a', 'directo', 'cut', 'williams', 'and', 'bobby', 'cannavale', 'as', 'jess', 'do', 'seem', 'initially', 'believable', 'as', 'a', 'couple', 'a', 'scene', 'or', 'two', 'establishing', 'their', 'relationship', 'might', 'have', 'helped', 'set', 'the', 'stage', 'otherwise', 'the', 'cast', 'is', 'exemplary', 'williams', 'offers', 'an', 'exceptionally', 'strong', 'characterization', 'and', 'not', 'a', 'gay', 'impersonation', 'sandra', 'oh', 'as', 'anna', 'joe', 'morton', 'as', 'ashe', 'and', 'rory', 'culkin', 'pete', 'logand', 'are', 'all', 'perfect', 'best', 'of', 'all', 'collett', 'donna', 'belongs', 'in', 'the', 'creepy', 'hall', 'of', 'fame', 'ms', 'oh', 'is', 'correct', 'in', 'saying', 'collette', 'might', 'be', 'you', 'know', 'like', 'that', 'guy', 'from', 'psycho', 'there', 'have', 'been', 'several', 'years', 'when', 'organizations', 'giving', 'acting', 'awards', 'seemed', 'to', 'reach', 'for', 'women', 'due', 'to', 'a', 'slighter', 'dispersion', 'of', 'roles', 'certainly', 'they', 'could', 'have', 'noticed', 'collette', 'with', 'some', 'award', 'consideration', 'she', 'is', 'that', 'good', 'and', 'director', 'patrick', 'stettner', 'definitely', 'evokes', 'hitchcock', 'he', 'even', 'makes', 'getting', 'a', 'sandwich', 'from', 'a', 'vending', 'machine', 'suspenseful', 'finally', 'writers', 'stettner', 'armistead', 'maupin', 'and', 'terry', 'anderson', 'deserve', 'gratitude', 'from', 'flight', 'attendants', 'everywhere', 'the', 'night', 'listener', '1', '21', '06', 'patrick', 'stettner', 'robin', 'williams', 'toni', 'collette', 'sandra', 'oh', 'rory', 'culkin'], tags=['train/pos/10006_7.txt']),\n",
       " TaggedDocument(words=['the', 'night', 'listener', '2006', '1', '2', 'robin', 'williams', 'toni', 'collette', 'bobby', 'cannavale', 'rory', 'culkin', 'joe', 'morton', 'sandra', 'oh', 'john', 'cullum', 'lisa', 'emery', 'becky', 'ann', 'baker', 'dir', 'patrick', 'stettner', 'hitchcockian', 'suspenser', 'gives', 'williams', 'a', 'stand', 'out', 'low', 'key', 'performance', 'what', 'is', 'it', 'about', 'celebrities', 'and', 'fans', 'what', 'is', 'the', 'near', 'paranoia', 'one', 'associates', 'with', 'the', 'other', 'and', 'why', 'is', 'it', 'almost', 'the', 'norm', 'in', 'the', 'latest', 'derange', 'fan', 'scenario', 'based', 'on', 'true', 'events', 'no', 'less', 'williams', 'stars', 'as', 'a', 'talk', 'radio', 'personality', 'named', 'gabriel', 'no', 'one', 'who', 'reads', 'stories', 'h', 'penned', 'over', 'the', 'airwaves', 'and', 'has', 'accumulated', 'an', 'interesting', 'fan', 'in', 'the', 'form', 'of', 'a', 'young', 'boy', 'named', 'pete', 'logand', 'culkin', 'who', 'has', 'submitted', 'a', 'manuscript', 'about', 'the', 'travails', 'of', 'his', 'troubled', 'youth', 'to', 'no', 'on', 'editor', 'ashe', 'morton', 'who', 'gives', 'it', 'to', 'no', 'one', 'to', 'read', 'for', 'himself', 'no', 'one', 'is', 'naturally', 'disturbed', 'but', 'ultimately', 'intrigued', 'about', 'the', 'nightmarish', 'existence', 'of', 'pete', 'being', 'abducted', 'and', 'sexually', 'abused', 'for', 'years', 'until', 'he', 'was', 'finally', 'rescued', 'by', 'a', 'nurse', 'named', 'donna', 'collette', 'giving', 'an', 'excellent', 'performance', 'who', 'has', 'adopted', 'the', 'boy', 'but', 'her', 'correspondence', 'with', 'no', 'one', 'reveals', 'that', 'pete', 'is', 'dying', 'from', 'aids', 'naturally', 'no', 'one', 'wants', 'to', 'meet', 'the', 'fans', 'but', 'is', 'suddenly', 'in', 'doubt', 'to', 'their', 'possibly', 'devious', 'ulterior', 'motives', 'when', 'the', 'seed', 'is', 'planted', 'by', 'his', 'estranged', 'lover', 'jess', 'cannavale', 'whose', 'sudden', 'departure', 'from', 'their', 'new', 'york', 'city', 'apartment', 'has', 'no', 'one', 'in', 'an', 'emotional', 'tailspin', 'that', 'has', 'only', 'now', 'grown', 'into', 'a', 'tempest', 'in', 'a', 'teacup', 'when', 'he', 'decides', 'to', 'do', 'some', 'investigating', 'into', 'donna', 'and', 'pet', 'backgrounds', 'discovering', 'some', 'truths', 'that', 'he', 'did', 'anticipate', 'written', 'by', 'armistead', 'maupin', 'who', 'co', 'wrote', 'the', 'screenplay', 'with', 'his', 'former', 'lover', 'terry', 'anderson', 'and', 'the', 'fil', 'novice', 'director', 'stettner', 'and', 'based', 'on', 'a', 'true', 'story', 'about', 'a', 'fa', 'hoax', 'found', 'out', 'has', 'some', 'hitchcockian', 'moments', 'that', 'run', 'on', 'full', 'tilt', 'like', 'any', 'good', 'old', 'fashioned', 'pot', 'boiler', 'does', 'it', 'helps', 'that', 'williams', 'gives', 'a', 'stand', 'out', 'low', 'key', 'performance', 'as', 'the', 'conflicted', 'good', 'hearted', 'personality', 'who', 'genuinely', 'wants', 'to', 'believe', 'that', 'his', 'number', 'one', 'fan', 'is', 'in', 'fact', 'real', 'and', 'does', 'love', 'him', 'the', 'one', 'thing', 'that', 'has', 'escaped', 'his', 'own', 'reality', 'and', 'has', 'some', 'unsettling', 'dreadful', 'moments', 'with', 'the', 'creepy', 'collette', 'whose', 'one', 'physical', 'trait', 'i', 'will', 'leave', 'unmentioned', 'but', 'underlines', 'the', 'desperation', 'of', 'her', 'character', 'that', 'can', 'rattle', 'you', 'to', 'the', 'core', 'however', 'the', 'film', 'runs', 'out', 'of', 'gas', 'and', 'eventually', 'becomes', 'a', 'bit', 'repetitive', 'and', 'predictable', 'despite', 'a', 'finely', 'directed', 'piece', 'of', 'hoodwink', 'and', 'mystery', 'by', 'stettner', 'it', 'pays', 'to', 'listen', 'to', 'your', 'own', 'inner', 'voice', 'be', 'careful', 'of', 'what', 'you', 'hope', 'for'], tags=['train/pos/10007_7.txt']),\n",
       " TaggedDocument(words=['you', 'know', 'robin', 'williams', 'god', 'bless', 'him', 'is', 'constantly', 'shooting', 'himself', 'in', 'the', 'foot', 'lately', 'with', 'all', 'these', 'dumb', 'comedies', 'he', 'has', 'done', 'this', 'decade', 'with', 'perhaps', 'the', 'exception', 'of', 'death', 'to', 'smoochy', 'which', 'bombed', 'when', 'it', 'came', 'out', 'but', 'is', 'now', 'a', 'cult', 'classic', 'the', 'dramas', 'he', 'has', 'made', 'lately', 'have', 'been', 'fantastic', 'especially', 'insomnia', 'and', 'one', 'hour', 'photo', 'the', 'night', 'listener', 'despite', 'mediocre', 'reviews', 'and', 'a', 'quick', 'dvd', 'release', 'is', 'among', 'his', 'best', 'work', 'period', 'this', 'is', 'a', 'very', 'chilling', 'story', 'even', 'though', 'it', 'does', 'include', 'a', 'serial', 'killer', 'or', 'anyone', 'that', 'physically', 'dangerous', 'for', 'that', 'matter', 'the', 'concept', 'of', 'the', 'film', 'is', 'based', 'on', 'an', 'actual', 'case', 'of', 'fraud', 'that', 'still', 'has', 'yet', 'to', 'be', 'officially', 'confirmed', 'in', 'high', 'school', 'i', 'read', 'an', 'autobiography', 'by', 'a', 'child', 'named', 'anthony', 'godby', 'johnson', 'who', 'suffered', 'horrific', 'abuse', 'and', 'eventually', 'contracted', 'aids', 'as', 'a', 'result', 'i', 'was', 'moved', 'by', 'the', 'story', 'until', 'i', 'read', 'reports', 'online', 'that', 'johnson', 'may', 'not', 'actually', 'exist', 'when', 'i', 'saw', 'this', 'movie', 'the', 'confused', 'feelings', 'that', 'robin', 'williams', 'so', 'brilliantly', 'portrayed', 'resurfaced', 'in', 'my', 'mind', 'toni', 'collette', 'probably', 'gives', 'her', 'best', 'dramatic', 'performance', 'too', 'as', 'the', 'ultimately', 'sociopathic', 'caretaker', 'her', 'role', 'was', 'a', 'far', 'cry', 'from', 'those', 'she', 'had', 'in', 'movies', 'like', 'little', 'miss', 'sunshine', 'there', 'were', 'even', 'times', 'she', 'looked', 'into', 'the', 'camera', 'where', 'i', 'thought', 'she', 'was', 'staring', 'right', 'at', 'me', 'it', 'takes', 'a', 'good', 'actress', 'to', 'play', 'that', 'sort', 'of', 'role', 'and', 'i', 'this', 'understated', 'yet', 'well', 'reviewed', 'role', 'that', 'makes', 'toni', 'collette', 'probably', 'one', 'of', 'the', 'best', 'actresses', 'of', 'this', 'generation', 'not', 'to', 'have', 'even', 'been', 'nominated', 'for', 'an', 'academy', 'award', 'as', 'of', '2008', 'i', 'incredible', 'that', 'there', 'is', 'at', 'least', 'one', 'woman', 'in', 'this', 'world', 'who', 'is', 'like', 'this', 'and', 'i', 'scary', 'too', 'this', 'is', 'a', 'good', 'dark', 'film', 'that', 'i', 'highly', 'recommend', 'be', 'prepared', 'to', 'be', 'unsettled', 'though', 'because', 'this', 'movie', 'leaves', 'you', 'with', 'a', 'strange', 'feeling', 'at', 'the', 'end'], tags=['train/pos/10008_7.txt'])]"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "unsup_sentences[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class PermuteSentences(object):\n",
    "    def __init__(self,sents):\n",
    "        self.sents = sents\n",
    "    \n",
    "    def __iter__(self):\n",
    "        shuffled = list(self.sents)\n",
    "        random.shuffle(shuffled)\n",
    "        for sent in shuffled:\n",
    "            yield sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "0000, processed 11262154 words (4930018/s), 111135 word types, 80000 tags\n",
      "2021-01-24 20:40:07,965 : INFO : PROGRESS: at example #90000, processed 12681119 words (4509386/s), 116790 word types, 90000 tags\n",
      "2021-01-24 20:40:08,260 : INFO : PROGRESS: at example #100000, processed 14070490 words (4722741/s), 121864 word types, 100000 tags\n",
      "2021-01-24 20:40:08,562 : INFO : PROGRESS: at example #110000, processed 15456291 words (4597954/s), 126783 word types, 110000 tags\n",
      "2021-01-24 20:40:08,850 : INFO : PROGRESS: at example #120000, processed 16866466 words (4918099/s), 131630 word types, 120000 tags\n",
      "2021-01-24 20:40:09,130 : INFO : PROGRESS: at example #130000, processed 18267205 words (5018584/s), 136435 word types, 130000 tags\n",
      "2021-01-24 20:40:09,419 : INFO : PROGRESS: at example #140000, processed 19712303 words (5015115/s), 140872 word types, 140000 tags\n",
      "2021-01-24 20:40:09,724 : INFO : PROGRESS: at example #150000, processed 21132997 words (4659809/s), 145114 word types, 150000 tags\n",
      "2021-01-24 20:40:10,001 : INFO : PROGRESS: at example #160000, processed 22518453 words (5011756/s), 149187 word types, 160000 tags\n",
      "2021-01-24 20:40:10,293 : INFO : PROGRESS: at example #170000, processed 23937869 words (4883185/s), 153264 word types, 170000 tags\n",
      "2021-01-24 20:40:10,455 : INFO : collected 155371 word types and 175321 unique tags from a corpus of 175321 examples and 24692409 words\n",
      "2021-01-24 20:40:10,456 : INFO : Loading a fresh vocabulary\n",
      "2021-01-24 20:40:10,584 : INFO : effective_min_count=5 retains 57730 unique words (37% of original 155371, drops 97641)\n",
      "2021-01-24 20:40:10,584 : INFO : effective_min_count=5 leaves 24536230 word corpus (99% of original 24692409, drops 156179)\n",
      "2021-01-24 20:40:10,731 : INFO : deleting the raw counts dictionary of 155371 items\n",
      "2021-01-24 20:40:10,736 : INFO : sample=0.001 downsamples 47 most-common words\n",
      "2021-01-24 20:40:10,737 : INFO : downsampling leaves estimated 18578065 word corpus (75.7% of prior 24536230)\n",
      "2021-01-24 20:40:10,790 : INFO : constructing a huffman tree from 57730 words\n",
      "2021-01-24 20:40:12,130 : INFO : built huffman tree with maximum node depth 22\n",
      "2021-01-24 20:40:12,244 : INFO : estimated required memory for 57730 words and 50 dimensions: 145177400 bytes\n",
      "2021-01-24 20:40:12,245 : INFO : resetting layer weights\n",
      "2021-01-24 20:40:51,837 : INFO : training model with 3 workers on 57730 vocabulary and 50 features, using sg=1 hs=1 sample=0.001 negative=5 window=5\n",
      "2021-01-24 20:40:52,848 : INFO : EPOCH 1 - PROGRESS: at 3.38% examples, 627356 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:40:53,860 : INFO : EPOCH 1 - PROGRESS: at 6.95% examples, 641904 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:40:54,869 : INFO : EPOCH 1 - PROGRESS: at 10.45% examples, 648200 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:40:55,884 : INFO : EPOCH 1 - PROGRESS: at 13.99% examples, 653435 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:40:56,887 : INFO : EPOCH 1 - PROGRESS: at 17.60% examples, 655255 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:40:57,892 : INFO : EPOCH 1 - PROGRESS: at 21.20% examples, 656228 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-24 20:40:58,898 : INFO : EPOCH 1 - PROGRESS: at 24.86% examples, 658945 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-24 20:40:59,912 : INFO : EPOCH 1 - PROGRESS: at 28.43% examples, 659660 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-24 20:41:00,929 : INFO : EPOCH 1 - PROGRESS: at 31.95% examples, 660365 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:01,938 : INFO : EPOCH 1 - PROGRESS: at 35.67% examples, 661679 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:02,955 : INFO : EPOCH 1 - PROGRESS: at 39.26% examples, 662844 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-24 20:41:03,969 : INFO : EPOCH 1 - PROGRESS: at 42.87% examples, 663631 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-24 20:41:04,969 : INFO : EPOCH 1 - PROGRESS: at 46.40% examples, 663080 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-24 20:41:05,981 : INFO : EPOCH 1 - PROGRESS: at 49.98% examples, 664117 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-24 20:41:06,986 : INFO : EPOCH 1 - PROGRESS: at 53.53% examples, 664715 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-24 20:41:07,990 : INFO : EPOCH 1 - PROGRESS: at 57.23% examples, 665475 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:08,991 : INFO : EPOCH 1 - PROGRESS: at 60.80% examples, 665681 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:09,994 : INFO : EPOCH 1 - PROGRESS: at 64.34% examples, 666306 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:10,996 : INFO : EPOCH 1 - PROGRESS: at 67.84% examples, 666195 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:11,997 : INFO : EPOCH 1 - PROGRESS: at 71.49% examples, 666788 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:13,023 : INFO : EPOCH 1 - PROGRESS: at 75.15% examples, 666853 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-24 20:41:14,036 : INFO : EPOCH 1 - PROGRESS: at 78.81% examples, 666931 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:15,039 : INFO : EPOCH 1 - PROGRESS: at 82.45% examples, 667022 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:16,042 : INFO : EPOCH 1 - PROGRESS: at 86.01% examples, 667155 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:17,052 : INFO : EPOCH 1 - PROGRESS: at 89.63% examples, 667153 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:18,072 : INFO : EPOCH 1 - PROGRESS: at 93.27% examples, 667398 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-24 20:41:19,092 : INFO : EPOCH 1 - PROGRESS: at 96.85% examples, 666792 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:19,953 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-24 20:41:19,956 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-24 20:41:19,962 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-24 20:41:19,963 : INFO : EPOCH - 1 : training on 24692409 raw words (18754778 effective words) took 28.1s, 667072 effective words/s\n",
      "2021-01-24 20:41:20,988 : INFO : EPOCH 2 - PROGRESS: at 3.10% examples, 565128 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:22,002 : INFO : EPOCH 2 - PROGRESS: at 6.30% examples, 577856 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:23,013 : INFO : EPOCH 2 - PROGRESS: at 9.88% examples, 609191 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:24,025 : INFO : EPOCH 2 - PROGRESS: at 13.40% examples, 620634 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:25,030 : INFO : EPOCH 2 - PROGRESS: at 16.63% examples, 621133 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:26,033 : INFO : EPOCH 2 - PROGRESS: at 19.88% examples, 618121 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:27,034 : INFO : EPOCH 2 - PROGRESS: at 23.30% examples, 618243 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-24 20:41:28,046 : INFO : EPOCH 2 - PROGRESS: at 26.88% examples, 624664 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-24 20:41:29,066 : INFO : EPOCH 2 - PROGRESS: at 30.54% examples, 630138 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-24 20:41:30,066 : INFO : EPOCH 2 - PROGRESS: at 34.16% examples, 634983 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:31,067 : INFO : EPOCH 2 - PROGRESS: at 37.83% examples, 639591 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:32,075 : INFO : EPOCH 2 - PROGRESS: at 41.55% examples, 642541 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:33,087 : INFO : EPOCH 2 - PROGRESS: at 45.23% examples, 645699 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:34,106 : INFO : EPOCH 2 - PROGRESS: at 48.92% examples, 648233 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-24 20:41:35,119 : INFO : EPOCH 2 - PROGRESS: at 52.59% examples, 650320 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:36,126 : INFO : EPOCH 2 - PROGRESS: at 56.25% examples, 652672 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:37,136 : INFO : EPOCH 2 - PROGRESS: at 59.87% examples, 654306 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:38,141 : INFO : EPOCH 2 - PROGRESS: at 63.52% examples, 655529 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:39,143 : INFO : EPOCH 2 - PROGRESS: at 67.16% examples, 657119 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:40,151 : INFO : EPOCH 2 - PROGRESS: at 70.80% examples, 658228 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-24 20:41:41,152 : INFO : EPOCH 2 - PROGRESS: at 74.55% examples, 659873 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:42,175 : INFO : EPOCH 2 - PROGRESS: at 78.31% examples, 660540 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:43,186 : INFO : EPOCH 2 - PROGRESS: at 81.95% examples, 661425 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:44,189 : INFO : EPOCH 2 - PROGRESS: at 85.55% examples, 661778 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-24 20:41:45,192 : INFO : EPOCH 2 - PROGRESS: at 89.16% examples, 662703 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:46,204 : INFO : EPOCH 2 - PROGRESS: at 92.89% examples, 663609 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:47,222 : INFO : EPOCH 2 - PROGRESS: at 96.57% examples, 664296 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:48,161 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-24 20:41:48,172 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-24 20:41:48,185 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-24 20:41:48,186 : INFO : EPOCH - 2 : training on 24692409 raw words (18753741 effective words) took 28.2s, 664673 effective words/s\n",
      "2021-01-24 20:41:49,205 : INFO : EPOCH 3 - PROGRESS: at 3.28% examples, 604487 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:50,205 : INFO : EPOCH 3 - PROGRESS: at 6.96% examples, 645112 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-24 20:41:51,229 : INFO : EPOCH 3 - PROGRESS: at 10.59% examples, 658513 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:52,231 : INFO : EPOCH 3 - PROGRESS: at 14.31% examples, 664848 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:53,237 : INFO : EPOCH 3 - PROGRESS: at 17.90% examples, 668928 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:54,254 : INFO : EPOCH 3 - PROGRESS: at 21.55% examples, 670927 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:55,267 : INFO : EPOCH 3 - PROGRESS: at 25.13% examples, 671175 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:56,268 : INFO : EPOCH 3 - PROGRESS: at 28.77% examples, 672789 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-24 20:41:57,270 : INFO : EPOCH 3 - PROGRESS: at 32.39% examples, 673217 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:41:58,282 : INFO : EPOCH 3 - PROGRESS: at 36.10% examples, 673060 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-24 20:41:59,291 : INFO : EPOCH 3 - PROGRESS: at 39.84% examples, 673753 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:00,295 : INFO : EPOCH 3 - PROGRESS: at 43.48% examples, 674099 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:01,301 : INFO : EPOCH 3 - PROGRESS: at 47.19% examples, 675444 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-24 20:42:02,311 : INFO : EPOCH 3 - PROGRESS: at 50.81% examples, 675664 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:03,313 : INFO : EPOCH 3 - PROGRESS: at 54.58% examples, 676732 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:04,316 : INFO : EPOCH 3 - PROGRESS: at 58.17% examples, 676808 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-24 20:42:05,331 : INFO : EPOCH 3 - PROGRESS: at 61.87% examples, 677274 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-24 20:42:06,335 : INFO : EPOCH 3 - PROGRESS: at 65.60% examples, 677833 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:07,339 : INFO : EPOCH 3 - PROGRESS: at 69.15% examples, 678227 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-24 20:42:08,345 : INFO : EPOCH 3 - PROGRESS: at 72.84% examples, 678793 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:09,348 : INFO : EPOCH 3 - PROGRESS: at 76.53% examples, 679348 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:10,370 : INFO : EPOCH 3 - PROGRESS: at 80.23% examples, 678923 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-24 20:42:11,383 : INFO : EPOCH 3 - PROGRESS: at 83.85% examples, 679163 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:12,390 : INFO : EPOCH 3 - PROGRESS: at 87.56% examples, 679091 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:13,393 : INFO : EPOCH 3 - PROGRESS: at 91.29% examples, 679416 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-24 20:42:14,397 : INFO : EPOCH 3 - PROGRESS: at 94.98% examples, 679334 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:15,399 : INFO : EPOCH 3 - PROGRESS: at 98.66% examples, 679862 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:15,754 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-24 20:42:15,759 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-24 20:42:15,767 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-24 20:42:15,768 : INFO : EPOCH - 3 : training on 24692409 raw words (18751540 effective words) took 27.6s, 680053 effective words/s\n",
      "2021-01-24 20:42:16,781 : INFO : EPOCH 4 - PROGRESS: at 3.30% examples, 594464 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:17,789 : INFO : EPOCH 4 - PROGRESS: at 7.02% examples, 638873 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-24 20:42:18,806 : INFO : EPOCH 4 - PROGRESS: at 10.74% examples, 650865 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:19,816 : INFO : EPOCH 4 - PROGRESS: at 14.30% examples, 658153 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:20,822 : INFO : EPOCH 4 - PROGRESS: at 17.88% examples, 660106 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:21,829 : INFO : EPOCH 4 - PROGRESS: at 21.56% examples, 660733 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:22,832 : INFO : EPOCH 4 - PROGRESS: at 25.18% examples, 662747 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:23,833 : INFO : EPOCH 4 - PROGRESS: at 28.77% examples, 664648 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:24,834 : INFO : EPOCH 4 - PROGRESS: at 32.23% examples, 664622 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:25,835 : INFO : EPOCH 4 - PROGRESS: at 35.81% examples, 665981 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-24 20:42:26,840 : INFO : EPOCH 4 - PROGRESS: at 39.47% examples, 667476 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-24 20:42:27,847 : INFO : EPOCH 4 - PROGRESS: at 43.11% examples, 667620 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:28,857 : INFO : EPOCH 4 - PROGRESS: at 46.79% examples, 668417 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:29,864 : INFO : EPOCH 4 - PROGRESS: at 50.48% examples, 668489 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:30,871 : INFO : EPOCH 4 - PROGRESS: at 54.08% examples, 668430 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-24 20:42:31,871 : INFO : EPOCH 4 - PROGRESS: at 57.58% examples, 668650 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:32,889 : INFO : EPOCH 4 - PROGRESS: at 61.02% examples, 666830 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:33,899 : INFO : EPOCH 4 - PROGRESS: at 64.74% examples, 667712 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:34,911 : INFO : EPOCH 4 - PROGRESS: at 67.70% examples, 662406 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:35,917 : INFO : EPOCH 4 - PROGRESS: at 71.22% examples, 662210 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:36,922 : INFO : EPOCH 4 - PROGRESS: at 74.86% examples, 662912 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:37,925 : INFO : EPOCH 4 - PROGRESS: at 78.56% examples, 663649 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-24 20:42:38,939 : INFO : EPOCH 4 - PROGRESS: at 82.16% examples, 664249 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-24 20:42:39,940 : INFO : EPOCH 4 - PROGRESS: at 85.68% examples, 665175 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:40,949 : INFO : EPOCH 4 - PROGRESS: at 89.03% examples, 663696 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:41,960 : INFO : EPOCH 4 - PROGRESS: at 92.64% examples, 663952 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:42,974 : INFO : EPOCH 4 - PROGRESS: at 96.34% examples, 664212 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:43,976 : INFO : EPOCH 4 - PROGRESS: at 99.91% examples, 664431 words/s, in_qsize 3, out_qsize 0\n",
      "2021-01-24 20:42:43,980 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-24 20:42:43,998 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-24 20:42:44,003 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-24 20:42:44,005 : INFO : EPOCH - 4 : training on 24692409 raw words (18753167 effective words) took 28.2s, 664307 effective words/s\n",
      "2021-01-24 20:42:45,014 : INFO : EPOCH 5 - PROGRESS: at 3.18% examples, 596834 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-24 20:42:46,018 : INFO : EPOCH 5 - PROGRESS: at 6.70% examples, 636887 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:47,023 : INFO : EPOCH 5 - PROGRESS: at 10.32% examples, 649693 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:48,026 : INFO : EPOCH 5 - PROGRESS: at 13.89% examples, 653262 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:49,027 : INFO : EPOCH 5 - PROGRESS: at 17.62% examples, 659760 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:50,037 : INFO : EPOCH 5 - PROGRESS: at 21.36% examples, 663395 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:51,039 : INFO : EPOCH 5 - PROGRESS: at 25.09% examples, 667338 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-24 20:42:52,049 : INFO : EPOCH 5 - PROGRESS: at 28.59% examples, 663317 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-24 20:42:53,054 : INFO : EPOCH 5 - PROGRESS: at 31.67% examples, 654150 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:54,060 : INFO : EPOCH 5 - PROGRESS: at 35.04% examples, 651132 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:55,080 : INFO : EPOCH 5 - PROGRESS: at 38.44% examples, 648445 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-24 20:42:56,084 : INFO : EPOCH 5 - PROGRESS: at 41.91% examples, 649027 words/s, in_qsize 6, out_qsize 0\n",
      "2021-01-24 20:42:57,093 : INFO : EPOCH 5 - PROGRESS: at 45.63% examples, 651327 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:58,093 : INFO : EPOCH 5 - PROGRESS: at 49.24% examples, 653084 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:42:59,101 : INFO : EPOCH 5 - PROGRESS: at 52.86% examples, 654926 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:43:00,106 : INFO : EPOCH 5 - PROGRESS: at 56.55% examples, 657215 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:43:01,107 : INFO : EPOCH 5 - PROGRESS: at 60.29% examples, 659710 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:43:02,109 : INFO : EPOCH 5 - PROGRESS: at 64.02% examples, 661997 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:43:03,125 : INFO : EPOCH 5 - PROGRESS: at 67.53% examples, 661114 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:43:04,128 : INFO : EPOCH 5 - PROGRESS: at 71.09% examples, 661559 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:43:05,128 : INFO : EPOCH 5 - PROGRESS: at 74.81% examples, 663479 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:43:06,131 : INFO : EPOCH 5 - PROGRESS: at 78.56% examples, 664494 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:43:07,154 : INFO : EPOCH 5 - PROGRESS: at 82.22% examples, 665496 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:43:08,162 : INFO : EPOCH 5 - PROGRESS: at 85.96% examples, 666739 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:43:09,164 : INFO : EPOCH 5 - PROGRESS: at 89.64% examples, 667547 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:43:10,176 : INFO : EPOCH 5 - PROGRESS: at 93.35% examples, 668320 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:43:11,179 : INFO : EPOCH 5 - PROGRESS: at 97.06% examples, 669650 words/s, in_qsize 5, out_qsize 0\n",
      "2021-01-24 20:43:11,968 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-01-24 20:43:11,985 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-01-24 20:43:11,991 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-01-24 20:43:11,993 : INFO : EPOCH - 5 : training on 24692409 raw words (18754300 effective words) took 28.0s, 670285 effective words/s\n",
      "2021-01-24 20:43:11,993 : INFO : training on a 123462045 raw words (93767526 effective words) took 140.2s, 669025 effective words/s\n"
     ]
    }
   ],
   "source": [
    "permuter = PermuteSentences(unsup_sentences)\n",
    "model = Doc2Vec(permuter,dm=0,hs=1,size=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.delete_temporary_training_data(keep_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-01-24 20:43:12,181 : INFO : saving Doc2Vec object under reviews.d2v, separately None\n",
      "2021-01-24 20:43:13,978 : INFO : saved reviews.d2v\n"
     ]
    }
   ],
   "source": [
    "model.save('reviews.d2v')\n",
    "# in other program, we could write: model = Doc2Vec.load('reviews.d2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 0.15615003,  0.35533318,  0.0244548 ,  0.31123462,  0.07109641,\n",
       "       -0.23480405,  0.21650085,  0.03178753,  0.28266463, -0.10115225,\n",
       "        0.12566638, -0.21731445,  0.21311566, -0.30245453, -0.01227137,\n",
       "       -0.2411961 , -0.23300245,  0.00300279,  0.11262076, -0.1246913 ,\n",
       "        0.00953154, -0.15050216,  0.02852447, -0.33485866, -0.07655568,\n",
       "       -0.04546369,  0.2395672 , -0.06979701, -0.02417124, -0.11960468,\n",
       "        0.05165534, -0.10228276,  0.01345713,  0.23048018, -0.15954658,\n",
       "       -0.07275184, -0.14637718,  0.5132226 ,  0.02547812, -0.03648876,\n",
       "        0.04452513,  0.02096366,  0.26291052, -0.032118  ,  0.31133208,\n",
       "       -0.1114691 ,  0.03198015,  0.24159124, -0.26068214, -0.05742063],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "model.infer_vector(extract_words('This place is not worth your time, let alone Vegas.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.56397027]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(\n",
    "    [model.infer_vector(extract_words('This place is not worth your time, let alone Vegas.'))],\n",
    "    [model.infer_vector(extract_words('Service sucks.'))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.30007932]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "cosine_similarity(\n",
    "    [model.infer_vector(extract_words('Highly recommended.'))],\n",
    "    [model.infer_vector(extract_words('Service sucks.'))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "sentvecs = []\n",
    "sentiments = []\n",
    "for fname in ['yelp','amazon_cells','imdb']:\n",
    "    with open('sentiment labelled sentences/%s_labelled.txt'% fname, encoding='UTF-8') as f:\n",
    "        for i,line in enumerate(f):\n",
    "            line_split = line.strip().split('\\t')\n",
    "            sentences.append(line_split[0])\n",
    "            words = extract_words(line_split[0])\n",
    "            sentvecs.append(model.infer_vector(words,steps=10)) # create a vector for this document\n",
    "            sentiments.append(int(line_split[1]))\n",
    "\n",
    "#shuffle sentences, sentvecs, sentiments together \n",
    "\n",
    "combined = list(zip(sentences,sentvecs,sentiments))\n",
    "random.shuffle(combined)\n",
    "sentences,sentvecs,sentiments=zip(*combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=9)\n",
    "clfrf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.8029999999999999, 0.003858612300930068)"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "scores = cross_val_score(clf,sentvecs, sentiments, cv=5)\n",
    "np.mean(scores),np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.7996666666666666, 0.01007747763855399)"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "scores = cross_val_score(clfrf,sentvecs,sentiments,cv=5)\n",
    "np.mean(scores),np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "pipeline = make_pipeline(CountVectorizer(),TfidfTransformer(),RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.7813333333333333, 0.018720161442798634)"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    " scores = cross_val_score(pipeline,sentences, sentiments,cv=5)\n",
    " np.mean(scores),np.std(scores)"
   ]
  }
 ]
}